CS225 Final project reflection

Have you ever seen the movie Project X? We would all have loved to have been at that party, but none of us would want to host it. When we host a party, we want to have strong connections to everyone there, but there is no easy way to compile such a list, as someone will always end up being excluded. One thing that includes everybody in today's day and age is social media. By taking in a dataset of twitter followers, we thought we could find the largest component of that group where everyone is well connected to each other in order to find the perfect group for a party. Additionally, we wanted a sort of “override” function where we would be able to include extraneous friends if we desired, while still maintaining everybody knowing at least some other people.

After some research we came to a few conclusions: 

The best way to define a collection of people who know each other well enough to make a good party is a strongly connected component.
The best algorithm to find all of those is Tarjan's because it does it in linear time with a fairly simple implementation centered around an SCC forming a subtree in a DFS scan.
The best way to maintain everyone knowing each other without inviting an extraneous person is to find a path between you and that extraneous person via Twitter followers who all know each other. Additionally, we should optimize so that the people linking you are the most social, which should make up for some of the deficit of them not already being part of the SCC.
A great pathfinding algorithm with directionality and edge weights is Dijkstra's, so that is our second algorithm to use.

Once we made these decisions, we had to come up with a few more solutions that were more low level that helped us implement it, the first of which was how to unpack and actually use the data. It comes in as a txt file containing nothing but a list of connections. Two numbers on each line, each representing a user, with the left following the right.

 We take these in and organize them into an adjacency list wherein we correlate each node to all of the other nodes who it is following.
We will have the edge weights be the “socialness”, which we will measure as the number of followers that a person has. We will calculate this for the node that is being followed, not the follower. 
An interesting problem we ran into quite quickly was that our dataset was far too large. It was extremely unwieldy the farther we got away from the user's ID. As such we decided to run BFS with a cap of 2 orders of separation away from the  initial user. Any farther than this and the runaway exponentiation led to us having millions of somewhat connected people. Our initial Idea was just to run a BFS as far as it could and then dump all of the disconnected components, but those ended up being less than 1% of our users.


Some of the things that we got experience with while working on this project include actually using GitHub/working in a group together for a CS project. Pushing to branches, creating pull requests, and merging were something of a foreign project until recently with this class and CS222. We also had to figure out how to segment our work since some things build off of each other and other parts are totally independent. Next, we learned how to write test cases without a full understanding of the final goal. In 128 when we wrote our own test cases, we knew exactly what the project was supposed to be, so it was very easy to reverse engineer what the test cases should have been. Here on the other hand, we didn’t really know where the project was going and had to test features individually. We also learned more generally how to approach problems without a final product already known. Another thing we learned was how to implement a user interface. For the requirement of the ability to modify the dataset, we added commands that could be used to modify our projects and include the outsiders which we will use for Dijkstra’s which we also hadn’t really needed to do for anything before. Again, in MPs we know what the end goal is with a very specific breakdown of what functions are needed, this on the other hand allows us to really explore and figure out how to solve a problem. We came up with every single thing from the ground up including data types, implementations and even which algorithms to use which really gave us a different understanding of what working in computer science will mean.

Ingestion didn’t really create any issues as it was quite a simple problem that we have all faced a number of times. The same applies for our BFS implementation. The three new things we worked on were weighting, Dijkstra’s, and Tarjan’s. For the weights and Dijkstra’s, we share a set of nodes. At first however, we tried populating the weights separately and that led to issues as we were only reading in the nodes that actually have followers. Instead we copied the entire list, but that led to a new issue; Dijkstra’s was taking the absolute worst possible path. Upon examination we realized that we had done the weighting backwards in that the more followers a person has, the higher their weight. Dijkstra’s is a shortest path algorithm and as such it should have been the opposite so we subtracted the number of followers from INT_MAX and that solved all of our weighting issues. Then, on Dijkstra’s we were getting errors every time we ran anything, until we realized that we had hard coded the start and end nodes, and we were testing it with a new much smaller dataset. Upon solving that problem, fixing our Dijkstra’s implementation ended up being a breeze. Tarjan’s algorithm was conceptually difficult, but as soon as we understood what we were trying to do the implementation ended up being quite simple.

BFS
![Image 12-12-22 at 3 20 PM](https://user-images.githubusercontent.com/98365406/207159453-1a7dc2f0-d843-42e6-a9a9-c4c389b95efe.png)

The dataset in its entirety was far too large to actually be usable, so we decided to run BFS with a range cap to get rid of the people who were too far away from the user. Anyone who was 3 degrees of separation away or farther would be cut off because social media following has a very high exponential growth rate. This “Friend of a Friend” policy also prevents one person who abuses social media, or one celebrity from skewing the data and influencing the party too much. We initially tested this on one of our miniaturized datasets with 5 nodes(see below) to check how many people it removed, which was only one. This made sense with our layout so we moved it up to the full size dataset. Here we ran into some issues because it still removed less than 1% of the total nodes, but once we ran it while periodically outputting who was still in the party, we came to realize that it was working correctly, there was just far too much overlap in the graph of connections. Another realization that we came to was that we dont really need to run the bfs minimization on small datasets, it is mainly just there as an optimization tool to avoid slowdowns from oversized datasets. As such we added a check so that bfs only runs on oversized datasets (i.e. 10000+ nodes). 

Dijkstra’s
![Screenshot 2022-12-12 at 3 09 00 PM](https://user-images.githubusercontent.com/98365406/207159028-f7b39ce8-4bc0-43be-bb1b-16dd8599ad51.jpeg)

Our implementation of Dijkstra’s algorithm ended up being done immediately after the user inputs their ID. We run through all of the nodes in the user graph and generate the best possible path to each one, storing it in 2 maps, one for distance, one for the path. This allows our slowest algorithm to be run completely upfront so the user can select external individuals at will with no load times for Dijkstra’s to go and map everything out. Our small test case sets(see below) were designed to test all of the edge cases that Dijkstra’s algorithm could run into, as such we ran it on those user groups and cross referenced all of the data ourselves. While doing this we realized that features where end users could access all of their options for extraneous invitees would be useful, so we added to the UI which we were developing concurrently, so now any user can access all of the relevant Dijkstra’s data before making their decisions on who they want to invite to the party.

Tarjan’s
![Image 12-12-22 at 3 14 PM](https://user-images.githubusercontent.com/98365406/207159431-5bd93e18-5202-4b92-8fd6-068f24cf8d34.png)

Tarjan’s algorithm was surprisingly easy to implement. We ran it on a small scale using the same mini-datasets which contain the edge cases we need(see below), and then on the full scale one. This is the only algorithm we implemented after our BFS trimming, so testing it on the full dataset actually presented some readable results. An interesting discovery we made at this point was that a large percentage of the people within that BFS run will form the strongly connected component, but hardly anyone who isn't in that SCC will have any other followers. This led to us implementing a response in the UI for if the user inputs an ID with no followers, as that will never be part of a strongly connected component. 

Testing Dataset
<img width="68" alt="Screen Shot 2022-12-12 at 3 37 32 PM" src="https://user-images.githubusercontent.com/98365406/207159915-a30946a8-02bb-4dff-a6e1-71f58105876c.png">
<img width="436" alt="Screen Shot 2022-12-12 at 3 37 52 PM" src="https://user-images.githubusercontent.com/98365406/207159980-059dbf63-839b-4bb8-a0c5-e140990fdde4.png">

The first image here is the list of connections that we have which correlates to the graph on the right. As you can see, node 1(a) is inaccessible from anyone else, but is connected to b and c. These two form their own SCC, as do D and E, but there are no back edges between those two groups. This gave us a fairly comprehensive set of cases to test against while also maintaining a compact and readable size for testing. Of course there were some errors when scaling up to the larger dataset so we tested against that as well, but most of the conceptual issues were weeded out at this size. After running with this smaller test case, we added a few more, sized between 5 and 20 nodes, and went back over all of the other functions with the updated set of tests. At this time we implemented the makefule fully so we were able to get good, functional, testing data like we get on MPs, and fleshed out our whole testing suite on all functions.

